---------------------------------   Preamble   --------------------------------
This same posting can be seen on GitHub:
https://github.com/fmv1992/data_analysis_and_interpretation_specialization/tree/course_04_machine_learning/04_machine_learning_for_data_analysis/assignment_01
It is easier to read if you read there!
-----------------------------------   Post   ----------------------------------
-------------------------------   Introduction   ------------------------------
Data set: R dataset: "Prices of 50,000 round cut diamonds"
    Description: "A dataset containing the prices and other attributes of
    almost 54,000 diamonds."
    Weblink:
        https://github.com/fmv1992/data_analysis_and_interpretation_specialization/blob/course_04_machine_learning/04_machine_learning_for_data_analysis/data_sets/r_data_sets/ggplot2/diamonds.csv

Research Question: How can we categorize diamonds in 'expensive' versus 'cheap'
taking the cut, color, clarity and mass into account using machine learning?

Explanatory variables:
    - Cut: quality of the cut.
    - Color: diamond color from J (worst) to D (best)
    - Clarity: a measurement of how clear the diamond is (I1 (worst), SI1, SI2,
      VS1, VS2, VVS1, VVS2, IF (best))
    - Mass: mass of the diamong (in kilograms)

Response variable:
    - 'price_expensive': is considered expensive if diamond has a value of 
    5324.25 USD or more.

Details:
	- All the categorical variables (cut, color and clarity) were mapped into numbers:
        - In all the categories the corrisponding number are from lower ->
          worst to greater -> best.

Notes:
    As said in the video 'Building a Decision Tree with Python' does not
    support pruning.

Goal: obtain a categorization of the 'price_expensive' variable considering
the: cut, color, clarity and mass variables.

Tip:
    - For the results to become reproducible we can give a random seed/state
      for the pseudo random number generators for both the train/test split and
      the decision tree classifier.

-------------------------------   Code Section   ------------------------------
                    ---------------   Code 1 --------------

u"""
Coursera Course: Machine Learning for Data Analysis.

    https://www.coursera.org/learn/machine-learning-data-analysis


This course is one in a series of: Data Analysis and Interpretation

Assignment 01: Decision Trees.
"""

# # pylama: skip=1
# pylama:ignore=C101,W0611

import pandas as pd
import numpy as np
import matplotlib.pylab as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
import sklearn.metrics
from assignment_01_data_preparation import return_processed_diamonds_data_set
import seaborn
import pydotplus
from IPython.display import Image
from io import StringIO
from sklearn import tree


def main():
    u"""Main function for assignment 01."""
    df = return_processed_diamonds_data_set()

    if df.isnull().sum().sum() != 0:
        raise ValueError('Your data has unintended nulls.')

    df.describe()

    # Split into training and testing sets
    input_variables = df[['cut', 'color', 'clarity', 'mass']]
    output_variable = df.price_expensive_binary  # Categorized price

    input_training, input_test, output_training, output_test = train_test_split(
        input_variables, output_variable, test_size=0.3, random_state=0)

    # Build model on training data
    tree_classifier = DecisionTreeClassifier(
        max_depth=4,
        min_samples_split=1000,
        random_state=0,  # This is to keep data reproducible.
    )

    tree_classifier = tree_classifier.fit(input_training, output_training)

    predictions = tree_classifier.predict(input_test)

    print('conf matrix', sklearn.metrics.confusion_matrix(output_test,
                                                          predictions))
    print('accuracy', sklearn.metrics.accuracy_score(output_test, predictions))

    # Displaying the decision tree
    # from StringIO import StringIO
    # from StringIO import StringIO
    # from IPython.display import Image
    # out = StringIO()
    # tree.export_graphviz(classifier, out_file=out)
    # import pydotplus
    # graph=pydotplus.graph_from_dot_data(out.getvalue())
    # Image(graph.create_png())

    out = StringIO()
    tree.export_graphviz(
        tree_classifier,
        out_file=out,
        feature_names=input_variables.columns.tolist(),
        class_names=['cheap', 'expensive'],
        filled=True,
        impurity=True,)
    graph = pydotplus.graph_from_dot_data(out.getvalue())
    with open('graph.png', 'wb') as f:
        f.write(graph.create_png())

    # Histogram of Clarity Variable
    plt.figure(0)
    df.clarity.value_counts(sort=False).plot.bar()
    plt.title('Histogram of Clarity values (0: least clear)')
    plt.tight_layout()
    plt.savefig('diamonds_clarity_histogram.png', dpi=500)
    plt.close()

    # Histogram of Cut Variable
    plt.figure(0)
    df.cut.value_counts(sort=False).plot.bar()
    plt.title('Histogram of Cut values (4: ideal)')
    plt.tight_layout()
    plt.savefig('diamonds_cut_histogram.png', dpi=500)
    plt.close()

    # Histogram of Color Variable
    plt.figure(0)
    df.color.value_counts(sort=False).plot.bar()
    plt.title('Histogram of Color values (0: worst)')
    plt.tight_layout()
    plt.savefig('diamonds_color_histogram.png', dpi=500)
    plt.close()


    # Histogram of the mass Variable
    plt.figure(0)
    df.mass.plot.hist()
    plt.title('Mass of the diamong (kg)')
    plt.tight_layout()
    plt.savefig('diamonds_mass_histogram.png', dpi=500)
    plt.close()

    return df


if __name__ == '__main__':
    main()
                    ---------------   Code 2 --------------
"""
Data preparation for assignment 01.

Prepares the diamond database found in

https://vincentarelbundock.github.io/Rdatasets/datasets.html

Some categorical variables are mapped to integers ranging from:
    0: worst value in that categorical
    ...
    max: best value in that categorical

"""
# # pylama:skip=1
# pylama:ignore=W:ignore=C101
import pandas as pd
import numpy as np


def return_processed_diamonds_data_set():
    u"""Main function. Return processed diamonds dataset."""
    # Set constants
    CSVPATH = '../data_sets/r_data_sets/ggplot2/diamonds.csv'
    DTYPES = {
        'carat': np.float64,
        'cut': pd.Categorical,
        'color': pd.Categorical,
        'clarity': pd.Categorical,
        'depth': np.float64,
        'table': np.float64,
        'price': np.int64,
        'x': np.float64,
        'y': np.float64,
        'z': np.float64,
    }
    PRICE_QUANTILE = 0.75

    # Load data set.
    df = pd.read_csv(CSVPATH, index_col=0, dtype=DTYPES)

    # Categorize expensive or cheap (our output variable).
    is_expensive = df.price > df.price.quantile(PRICE_QUANTILE)
    print(
        'The {0} quantile for price is {1} USD. There are a total of {2} '
        '({3:1.1%}) diamonds which are expensive.'.format(
            PRICE_QUANTILE,
            df.price.quantile(PRICE_QUANTILE),
            is_expensive.sum(),
            is_expensive.sum()/df.price.shape[0]))
    df['price_expensive'] = pd.Categorical.from_codes(
        is_expensive,
        categories=['cheap', 'expensive'])
    df['price_expensive_binary'] = df.price_expensive.map(
        {'expensive': 1, 'cheap': 0})

    # Helper to categorize the clarity.
    clarity_map = {
        'I1': 0,    # Least clear
        'SI1': 1,
        'SI2': 2,
        'VS1': 3,
        'VS2': 4,
        'VVS1': 5,
        'VVS2': 6,
        'IF': 7,    # Most clear
    }
    df.replace(to_replace={'clarity': clarity_map}, inplace=True)

    # Helper to categorize color.
    # 0 is the worst color (J)
    # 6 is the best color (D)
    df.color = df.color.apply(lambda x: ord('j') - ord(x.lower()))

    # Helper to categorize cut
    df.cut = df.cut.apply(str.lower)
    CUT_SCALE = {
        'fair': 0,
        'good': 1,
        'very good': 2,
        'premium': 3,
        'ideal': 4,
    }
    df.replace(to_replace={'cut': CUT_SCALE}, inplace=True)

    # Creates a column with SI mass
    CARAT_TO_KG = 200e-3
    df['mass'] = df.carat * CARAT_TO_KG

    # Gives a nice description for each variable.
    description = {
        'price': 'price in US dollars (\$326–\$18,823)',
        'carat': 'weight of the diamond (0.2–5.01)',
        'cut': 'quality of the cut (Fair, Good, Very Good, Premium, Ideal)',
        'colour': 'diamond colour, from J (worst) to D (best)',
        'color': 'diamond colour, from J (worst) to D (best)',
        'clarity': 'a measurement of how clear the diamond is (I1 (worst), '
            'SI1, SI2, VS1, VS2, VVS1, VVS2, IF (best))', # noqa
        'x': 'length in mm (0–10.74)',
        'y': 'width in mm (0–58.9)',
        'z': 'depth in mm (0–31.8)',
        'depth': 'total depth percentage = z / mean(x, y) = 2 * z / (x + y) '
            '(43–79)', # noqa
        'table': 'width of top of diamond relative to widest point (43–95)',
    }
    df.variable_description = description

    return df


if __name__ == '__main__':
    return_processed_diamonds_data_set()
                    -----------   Code Output   -----------
The 0.75 quantile for price is 5324.25 USD. There are a total of 13485 (25.0%) diamonds which are expensive.

conf matrix [[11893   263]
 [  405  3621]]

accuracy 0.958719564949

                  -----------   Files Generated   -----------

- graph.png: the decision tree output.

- diamonds_color_histogram.png: a histogram of the diamonds by color.
- diamonds_mass_histogram.png: a histogram of the diamonds by mass.
- diamonds_cut_histogram.png: a histogram of the diamonds by cut.
- diamonds_clarity_histogram.png: a histogram of the diamonds by clarity.

----------------------   Interpretation Of The Results   ----------------------

A decision tree was used to gather insight on the 'price_expensive' variable
(see above). The objective was to consider a diamond's cut, color, clarity and
mass ('weight') to determine if it has a great financial value or not.

Decision tree learning is a subfield in the Machine Learning field. Its
ultimate aim is to correctly classificate an output variable based on several
input variables. 

The first and the second variables to define the split between being expensive
or not was mass. This indicates the importance of such variable in determining
the price of a diamond. There are a total of 5 nodes which use this variable to
determine the split. This reinforces the importances of mass as a variable to
determine diamond price. Also the split goes along the intuition that the
higher the mass the more expensive the diamond shoud be (in this case the more
the likelihood of being categorized as expensivel).

Most of the other nodes were based on clarity (a total of 7). Again the
separation goes along with our intution that the more clear the diamond the
more expensive it tends to be.

There was just one node which differentiated by color. It yielded a small gini
coefficient leaf node and a big gini coef. leaf node  which indicates hat the
separation was ok by this variable. In hindsight since this variable
contributes poorly to the overall classification, it could be discarded.

Surprisingly enough the variable cut was not used to classificate the diamonds.
Therefore it could be considered of small relevance in classificating this data
set.

The confusion matrix had an accuracy of more than 95%. We can conclude that we
were successful in our classification. There were 405 false positives, that is:
there were 405 unexpensive diamonds which were classified as expensive.

An application of this program could be a machine based classificator/evaluator
of diamond prices. The user could just input the diamond's characteristics and
the computer could classificate it without human intervention. This could save
up on labor costs for example.

The sensitivity was of SE = (true positives) / (all positives) = 96% and the
specificity SP = (true negatives) / (all negatives) = 93%.

-----------------------------   Further Reading   -----------------------------
Decision trees:
    - http://scikit-learn.org/stable/modules/tree.html
