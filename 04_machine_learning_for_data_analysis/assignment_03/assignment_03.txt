---------------------------------   Preamble   --------------------------------
This same posting can be seen on GitHub:
https://github.com/fmv1992/data_analysis_and_interpretation_specialization/blob/course_04_machine_learning/04_machine_learning_for_data_analysis/assignment_03/assignment_03.txt
It is easier to read if you read there!
-----------------------------------   Post   ----------------------------------
-------------------------------   Introduction   ------------------------------
Data set: R dataset: "Prices of 50,000 round cut diamonds"
    - Description: "A dataset containing the prices and other attributes of
    almost 54,000 diamonds."
    - Weblink:
        https://github.com/fmv1992/data_analysis_and_interpretation_specialization/blob/course_04_machine_learning/04_machine_learning_for_data_analysis/data_sets/r_data_sets/ggplot2/diamonds.csv

Research Question: What are the important variables to do a linear regression
for diamond pricing?

Explanatory variables:
    - Carat: weight of the diamond (0.2–5.01)
    - Cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal)
        - is_cut_ideal: binary variable (True if the cut is equal to ideal)
        - is_cut_premium: similar as above.
        - is_cut_good: similar as above.
        - is_cut_very_good: similar as above.
        - is_cut_fair: similar as above.
    - Colour: diamond colour, from J (worst) to D (best)
    - Clarity: a measurement of how clear the diamond is (I1 (worst), SI1, SI2,
      VS1, VS2, VVS1, VVS2, IF (best))
    - X: length in mm (0–10.74)
    - Y: width in mm (0–58.9)
    - Z: depth in mm (0–31.8)
    - Depth: total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43–79)
    - Table: width of top of diamond relative to widest point (43–95)
    - Mass: mass of the diamong (kg)

Response variable:
    - Price: the price of the diamond in USD.

Methodology: A linear LASSO regression using the Least Angle Regression
algorithm was performed in a total of 15 input variables. This regression was
also cross validated using the k-fold cross validation method (k=10).

Details:
    - All the categorical variables (cut, color and clarity) were mapped into
      numbers:
        - In all the categories the corrisponding number are from lower ->
          worst to greater -> best.

Notes:
    - The variables were scaled to have a mean of zero and an SD = 1. (so the
      final binary variables do not have the 0 and 1 value anymore).

Tip:
    - For the results to become reproducible we can give a random seed/state
      for the pseudo random number generators for both the train/test split and
      the decision tree classifier.

-----------------------------------   Theory   --------------------------------
# LINEAR REGRESSION and OLS:
    Linear regression is a technique which tries to predict a response variable
    based on a weighted sum of input varibles:

    y = w_1 * x_1 + w_2 * x_2 + ... + w_n * x_n

    The Ordinary Least Square regression computes those 'w' weights minimizing
    the function f:

    f = sum ( ( y_i - w_1 * x_1_i + w_2 * x_2_i + ... + w_n * x_n_i ) ** 2 )

    where _i is the index for the ith observation.

# LASSO:
    LASSO (least absolute shrinkage and selection operator) is a technique for
    variable selection and regularization . This enhances model
    interpratability and prevents overfitting as you end up with a smaller set
    of variables to interpret and fit.
    Lasso adds up a term to the f function above which is:

    added_term = alpha * ( sum ( abs(w_i) ) )

    Thus adjusting the alpha value above increases or decreases the importance
    of this term in comparison to the OLS term (f above). For completeness, the
    LASSO regression minimizes the function:

    f_lasso = sum ( ( y_i - w_1 * x_1_i + w_2 * x_2_i + ... + w_n * x_n_i ) **
    2 ) + alpha * ( sum ( abs(w_i) ) )

    for a given alpha. Notice that for alpha = 0 then you have an OLS problem.

    Also notice that this is just a modelling setting; it does not give you an
    algorithm or method to actually minimnize f_lasso.

# LARS:
    Finally the LARS method gives a possible algorithm for minimizing the LASSO
    problem. It has a couple of benefits which can be checked out here (too
    technical for the scope of this assignment) (see:
    http://scikit-learn.org/stable/modules/linear_model.html#lars-lasso).

# CROSS VALIDATION:
    It is a statistical technique to tackle the problem of overfitting and
    validation. Instead of the single 'training group' and 'test group' in the
    machine learning context, cross validation attempts to create more groups
    for both testing and training.
    On the k-fold CV the universe of observations is divided into k groups.
    Each of this group is then used to test the algorithm and the training is
    conducted in the remaining k-1 folds. The average of the results (the
    weights in the context of linear regression) is then used as the final
    result.
    The advantage is that each observation is used only once for validation.
----------------------   Interpretation Of The Results   ----------------------
# LASSO:
    The LassoLarsCV regression was succesful in sieving the variables. The
    result of the linear model sieved out the 'is_cut_very_good',
    'is_cut_premium' and 'y' variables (that is their weights are equal to
    zero).

    That is to say that in the linear context it does not matter wheter the
    diamond receives a very good cut and its width. It is also worth noticing
    that the depth variable ('z') got a multiplier close to zero.
    Our conclusion is that again, in the linear context, it does not matter
    wheter the diamond receives a very good cut or premium cut, nor does it
    matter its width and depth. Their correlation to a diamond's price is not
    existent.

# LINEAR MODEL AND IMPORTANT VARIABLES:
    For the third time the most important variable for a diamond's price
    estimation was mass (this also happened in the assignments before these).
    Its weight was way higher than the other variables (more than five times
    the weight of the second variable). We can see that there is a positive
    correlation between a diamonds' mass and its price.

    The second most important variable was 'x': the diamond's length. It has
    a strong negative correlation with the diamond's price. Considering that
    the others diamond's dimension variables were not relevant we can assume
    that a 'lenghty' diamond has a smaller price than a 'unlenghty' one. This
    suggests that the more spherical shaped diamonds probably have more value
    (since its x, y and z dimensions are either unimportant or have negative
    importance in determining its price).

    The other most important variables (the ones most strongly correlated with
    our response variables) were clarity and color. Both are on a scale of
    increasing values according to their valuation impact. Thus both correctly
    displayed a positive correlation with the diamond's price.

# MEAN SQUARED ERROR AND R-SQUARED:
    We can notice from the figure of mean squared error for each fold that it
    was decreasing as the cross validation helped choose the best alpha for
    fitting. The tightness of all the fold's curves indicates that the data set
    is large and its division in ten parts yields parts that represent its
    entirety in a consistent way.

    Finally our R-squared for the test data set was around 89% which means that
    89% of the variance in the response variable can be explained by the input
    variables. An R-squared in this case is very good as the pricing of the
    diamonds is not a phyisical measurement but rather a result of human
    judgement. This is true for other variables as well (such as color and cut
    which are a result of human judgment).

# CONCLUSION:
I conclude that a linear model taking into account the variables:
 - mass
 - x
 - clarity
 - color
 - is_cut_fair
 - depth
 - table
 - is_cut_good
 - is_cut_ideal
 - z
 - is_cut_premium
 - y
 - is_cut_very_good
is a fair model in predicting the price of a diamond and that LASSO regression
was useful in discarding variables in this case.
-------------------------------   Code Section   ------------------------------
---------------   Code 1 --------------
"""
Data preparation for assignment 03.

Prepares the diamond database found in

https://vincentarelbundock.github.io/Rdatasets/datasets.html

Some categorical variables are mapped to integers ranging from:
    0: worst value in that categorical
    ...
    max: best value in that categorical

"""

# pylama:ignore=W0611
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


def return_processed_diamonds_data_set():
    u"""Main function. Return processed diamonds dataset."""
    # Set constants
    CSVPATH = '../data_sets/r_data_sets/ggplot2/diamonds.csv'
    DTYPES = {
        'carat': np.float64,
        'cut': pd.Categorical,
        'color': pd.Categorical,
        'clarity': pd.Categorical,
        'depth': np.float64,
        'table': np.float64,
        'price': np.int64,
        'x': np.float64,
        'y': np.float64,
        'z': np.float64,
    }
    PRICE_QUANTILE = 0.75

    # Load data set.
    df = pd.read_csv(CSVPATH, index_col=0, dtype=DTYPES)

    # Categorize expensive, medium or cheap (our output variable).
    is_expensive = df.price > df.price.quantile(PRICE_QUANTILE)
    df['price_expensive'] = pd.Categorical.from_codes(
        is_expensive,
        categories=['cheap', 'expensive'])
    df['price_expensive_binary'] = df.price_expensive.map(
        {'expensive': 1, 'cheap': 0})

    # Helper to categorize the clarity.
    clarity_map = {
        'I1': 0,    # Least clear
        'SI1': 1,
        'SI2': 2,
        'VS1': 3,
        'VS2': 4,
        'VVS1': 5,
        'VVS2': 6,
        'IF': 7,    # Most clear
    }
    df.replace(to_replace={'clarity': clarity_map}, inplace=True)

    # Helper to categorize color.
    # 0 is the worst color (J)
    # 6 is the best color (D)
    df.color = df.color.apply(lambda x: ord('j') - ord(x.lower()))

    # Helper to categorize cut
    df.cut = df.cut.apply(str.lower)
    # CUT_SCALE = {
    #     'fair': 0,
    #     'good': 1,
    #     'very good': 2,
    #     'premium': 3,
    #     'ideal': 4,
    # }
    # df.replace(to_replace={'cut': CUT_SCALE}, inplace=True)

    # Creates a column with SI mass
    CARAT_TO_KG = 200e-3
    df['mass'] = df.carat * CARAT_TO_KG

    # Gives a nice description for each variable.
    description = {
        'price': 'price in US dollars (\$326–\$18,823)',
        'carat': 'weight of the diamond (0.2–5.01)',
        'cut': 'quality of the cut (Fair, Good, Very Good, Premium, Ideal)',
        'colour': 'diamond colour, from J (worst) to D (best)',
        'color': 'diamond colour, from J (worst) to D (best)',
        'clarity': 'a measurement of how clear the diamond is (I1 (worst), '
            'SI1, SI2, VS1, VS2, VVS1, VVS2, IF (best))', # noqa
        'x': 'length in mm (0–10.74)',
        'y': 'width in mm (0–58.9)',
        'z': 'depth in mm (0–31.8)',
        'depth': 'total depth percentage = z / mean(x, y) = 2 * z / (x + y) '
            '(43–79)', # noqa
        'table': 'width of top of diamond relative to widest point (43–95)',
    }
    df.variable_description = description

    return df


def transform_cut_scale_to_category_columns(df):
    u"""Transform the cut scale to category columns.

    In order to create a couple of variables transform the cut scale to binary
    columns.

    """
    for cut_type in df.cut.unique():
        cut_type = cut_type.replace(' ', '_')
        df['is_cut_' + cut_type] = (df.cut == cut_type).astype('float64')
    df.drop('cut', axis=1, inplace=True)
    return df


def return_proc_and_transf_data_set():
    u"""Combine the two functions above."""
    return transform_cut_scale_to_category_columns(
        return_processed_diamonds_data_set())

if __name__ == '__main__':
    return_proc_and_transf_data_set()
---------------   Code 2 --------------
u"""
Coursera Course: Machine Learning for Data Analysis.

    https://www.coursera.org/learn/machine-learning-data-analysis


This course is one in a series of: Data Analysis and Interpretation

Assignment 03: Lasso Regression.
"""

# # pylama: skip=1
# pylama:ignore=C101,W0611

from pprint import pprint
import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns

# sklearn imports
from sklearn.model_selection import train_test_split
import sklearn.metrics
from sklearn import preprocessing
from sklearn.linear_model import LassoLarsCV
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_error

from assignment_03_data_preparation import return_proc_and_transf_data_set


def print_separator():
    u"""Prints a separator from between sections of text."""
    quantity = 50
    print(quantity*'-')
    return None


def main():
    u"""Main function for assignment 03."""
    # Load prepared data.
    df = return_proc_and_transf_data_set()
    # Mass is already included as mass in SI units.
    df.drop(['carat'], inplace=True, axis=1)
    # Those are dummy variables not needed in our data set anymore.
    df.drop(['price_expensive', 'price_expensive_binary'], inplace=True,
    axis=1)

    # A bit of error checking.
    if df.isnull().sum().sum() != 0:
        raise ValueError('Your data has unintended nulls.')

    # Cast our dataframe into float type.
    df = df.astype('float64')

    # Scale our dataframe to avoid the sparsity control of our dataframe biased
    # against some variables.
    print('Prior to scaling:')
    print(df.describe())
    df = df.apply(preprocessing.scale)
    print('After scaling:')
    print(df.describe())
    print_separator()
    if (df.mean().abs() > 1e-3).sum() > 0:
        raise ValueError('Scaling of your dataframe went wrong.')

    # Split into training and testing sets
    # The predictirs should not include any price variable since this was used
    # to create the output variable
    predictors = [x for x in df.columns.tolist() if 'price' not in x]
    print('Input variables:')
    pprint(predictors, indent=4)
    input_variables = df[predictors].copy()
    output_variable = df.price.copy()  # Categorized price
    print_separator()

    input_training, input_test, output_training, output_test
    = train_test_split(
        input_variables, output_variable, test_size=0.3, random_state=0)

    # A few words about the LassoLarsCV:

        # LASSO: least absolute shrinkage and selection operator (discussed in
        # the course material.

        # LARS: least angle regression: algorithm for linear regression models
        # to high-dimensional data (aka 'a lot of categories').
        # Compared to simple LASSO this model uses the LARS algorithm instead
        of
        # the 'vanilla' 'coordinate_descent' of simple LASSO.

        # CV: cross validation: this sets the alpha parameter (refered to as
        # lambda parameter in the course video) by cross validation.
        # In the simple LARS this alpha (the penalty factor) is an input of the
        # function.
        # 'The alpha parameter controls the degree of sparsity of the
        # coefficients estimated.
        # If alpha = zero then the method is the same as OLS.

    model = LassoLarsCV(
        cv=10,  # Number of folds.
        precompute=False,  # Do not precompute Gram matrix.
        # precompute=True,  # Do not precompute Gram matrix.
        # verbose=3,
    ).fit(input_training, output_training)

    dict_var_lin_coefs = dict(zip(
        predictors,
        model.coef_))

    print('Result of linear model:')
    pprint(sorted([(k, v) for k, v in dict_var_lin_coefs.items()],
                  key=lambda x: abs(x[1]))
           )
    print_separator()

    # Plot coefficient progression.
    # TODO: plot those on 4 different subplots.
    model_log_alphas = -np.log10(model.alphas_)
    ax = plt.gca()
    plt.plot(model_log_alphas, model.coef_path_.T)
    plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',
                label='alpha CV')
    plt.ylabel('Regression Coefficients')
    plt.xlabel('-log(alpha)')
    plt.title('Regression Coefficients Progression for Lasso Paths')
    plt.legend(predictors,
        loc='best',)
    plt.tight_layout()
    plt.savefig('result00.png', dpi=600)
    plt.close()
    # TODO: why are the coefficients in the result very different than the
    # coefficient path?
    #
    # There seems to be a scaling of the coefficient paths with an arbitrary
    # almost the same constant (194 in this case)
    #
    # print('Resulting alpha is not different than path alpha (difference):')
    # difference = model.alpha_ - model.alphas_
    # pprint(model.alpha_ - model.alphas_)
    # print('Resulting coefficients are very different than path coefficients
    (difference):')
    # pprint(model.coef_ - model.coef_path_.T)
    # print_separator()


    # Plot mean square error for each fold.
    # To avoid getting dividebyzero warning map zero to an extremely low value.
    model.cv_alphas_ = list(
        map(lambda x: x if x != 0 else np.inf,
            model.cv_alphas_))
    model_log_alphas = -np.log10(model.cv_alphas_)
    plt.figure()
    plt.plot(model_log_alphas, model.cv_mse_path_, ':')
    plt.plot(model_log_alphas, model.cv_mse_path_.mean(axis=-1), 'k',
            label='Average across the folds', linewidth=2)
    plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',
                label='alpha CV')
    plt.xlabel('-log(alpha)')
    plt.ylabel('Mean squared error')
    plt.title('Mean squared error on each fold')
    plt.legend()
    plt.tight_layout()
    plt.savefig('result01.png', dpi=600)
    plt.close()

    # Mean squared error of our model.
    train_error = mean_squared_error(output_training,
                                     model.predict(input_training))
    test_error = mean_squared_error(output_test,
                                    model.predict(input_test))
    print ('Training data MSE')
    print(train_error)
    print ('Test data MSE')
    print(test_error)
    print_separator()


    # R-square from training and test data.
    rsquared_train = model.score(
        input_training,
        output_training)
    rsquared_test = model.score(
        input_test,
        output_test)
    print ('Training data R-square')
    print(rsquared_train)
    print ('Test data R-square')
    print(rsquared_test)
    print_separator()

    return {'model': model, 'dataframe': df}


if __name__ == '__main__':
    main()
-----------   Code Output   -----------
Prior to scaling:
              color       clarity         depth         table         price  \
count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   
mean       3.405803      3.081183     61.749405     57.457184   3932.799722   
std        1.701105      1.769445      1.432621      2.234491   3989.439738   
min        0.000000      0.000000     43.000000     43.000000    326.000000   
25%        2.000000      1.000000     61.000000     56.000000    950.000000   
50%        3.000000      3.000000     61.800000     57.000000   2401.000000   
75%        5.000000      4.000000     62.500000     59.000000   5324.250000   
max        6.000000      7.000000     79.000000     95.000000  18823.000000   

                  x             y             z          mass  is_cut_ideal  \
count  53940.000000  53940.000000  53940.000000  53940.000000  53940.000000   
mean       5.731157      5.734526      3.538734      0.159588      0.399537   
std        1.121761      1.142135      0.705699      0.094802      0.489808   
min        0.000000      0.000000      0.000000      0.040000      0.000000   
25%        4.710000      4.720000      2.910000      0.080000      0.000000   
50%        5.700000      5.710000      3.530000      0.140000      0.000000   
75%        6.540000      6.540000      4.040000      0.208000      1.000000   
max       10.740000     58.900000     31.800000      1.002000      1.000000   

       is_cut_premium   is_cut_good  is_cut_very_good   is_cut_fair  
count    53940.000000  53940.000000           53940.0  53940.000000  
mean         0.255673      0.090953               0.0      0.029848  
std          0.436243      0.287545               0.0      0.170169  
min          0.000000      0.000000               0.0      0.000000  
25%          0.000000      0.000000               0.0      0.000000  
50%          0.000000      0.000000               0.0      0.000000  
75%          1.000000      0.000000               0.0      0.000000  
max          1.000000      1.000000               0.0      1.000000  
After scaling:
              color       clarity         depth         table         price  \
count  5.394000e+04  5.394000e+04  5.394000e+04  5.394000e+04  5.394000e+04   
mean   1.338360e-16  3.899159e-17 -3.996902e-15  9.695207e-17 -9.273676e-17   
std    1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00   
min   -2.002131e+00 -1.741344e+00 -1.308760e+01 -6.470073e+00 -9.040952e-01   
25%   -8.264134e-01 -1.176189e+00 -5.231053e-01 -6.521385e-01 -7.476808e-01   
50%   -2.385547e-01 -4.588080e-02  3.531678e-02 -2.046051e-01 -3.839672e-01   
75%    9.371628e-01  5.192735e-01  5.239361e-01  6.904618e-01  3.487866e-01   
max    1.525021e+00  2.214736e+00  1.204139e+01  1.680167e+01  3.732438e+00   

                  x             y             z          mass  is_cut_ideal  \
count  5.394000e+04  5.394000e+04  5.394000e+04  5.394000e+04  5.394000e+04   
mean   2.782103e-16 -8.430615e-17 -2.002271e-16 -9.695207e-17 -3.056098e-17   
std    1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00  1.000009e+00   
min   -5.109120e+00 -5.020931e+00 -5.014556e+00 -1.261458e+00 -8.157084e-01   
25%   -9.103248e-01 -8.882800e-01 -8.909461e-01 -8.395232e-01 -8.157084e-01   
50%   -2.777553e-02 -2.147398e-02 -1.237618e-02 -2.066210e-01 -8.157084e-01   
75%    7.210542e-01  7.052421e-01  7.103184e-01  5.106683e-01  1.225928e+00   
max    4.465203e+00  4.654965e+01  4.004758e+01  8.886075e+00  1.225928e+00   

       is_cut_premium   is_cut_good  is_cut_very_good   is_cut_fair  
count    5.394000e+04  5.394000e+04           53940.0  5.394000e+04  
mean     2.107654e-17 -1.053827e-17               0.0 -7.376788e-18  
std      1.000009e+00  1.000009e+00               0.0  1.000009e+00  
min     -5.860849e-01 -3.163116e-01               0.0 -1.754032e-01  
25%     -5.860849e-01 -3.163116e-01               0.0 -1.754032e-01  
50%     -5.860849e-01 -3.163116e-01               0.0 -1.754032e-01  
75%      1.706238e+00 -3.163116e-01               0.0 -1.754032e-01  
max      1.706238e+00  3.161440e+00               0.0  5.701149e+00  
--------------------------------------------------
Input variables:
[   'color',
    'clarity',
    'depth',
    'table',
    'x',
    'y',
    'z',
    'mass',
    'is_cut_ideal',
    'is_cut_premium',
    'is_cut_good',
    'is_cut_very_good',
    'is_cut_fair']
--------------------------------------------------
Result of linear model:
[('is_cut_very_good', 0.0),
 ('y', 0.0),
 ('is_cut_premium', 0.0),
 ('z', -0.003705584139703589),
 ('is_cut_ideal', 0.013088223883036492),
 ('is_cut_good', -0.015069312234053587),
 ('table', -0.01997493032988144),
 ('depth', -0.024344910786994991),
 ('is_cut_fair', -0.045974041080026257),
 ('color', 0.12114617407023132),
 ('clarity', 0.16174829776721228),
 ('x', -0.2253464011664984),
 ('mass', 1.2350540863482977)]
--------------------------------------------------
Training data MSE
0.105142296926
Test data MSE
0.101769955866
--------------------------------------------------
Training data R-square
0.89483580679
Test data R-square
0.898279462208
--------------------------------------------------
