------------------------------------------------------------   CUT ME HERE TO SUBMIT ON GITHUB   -------------------------------------------------------------
-----------------------------------   Post   ----------------------------------
---------------------------------   Summary   ---------------------------------
Data set: GapMinder

For the GapMinder dataset the quantitative variables with the greater strenght are:
	- positive correlation: 'incomeperperson', 'internetuserate' with pearsons_r = 0.823
	- negative correlation: 'hivrate', 'lifeexpectancy' with pearsons_r = -0.542
-------------------------------   Introduction   ------------------------------
In this assignment I used the GapMinder dataset to exhaust comparisons between the quantitative columns.
Well... Since we are using programs it is possible to automate such a task. Therefore I used itertools to combine all the 15 columns in combinations of 2. That resulted in 105 comparisons which were sorted by their pearsons_r value.

With this procedure it is easy to find the highest correlation as well as the lowest.

I decided to interpret and give a few words on four groups:
	- the 1st highest positive correlation (high pearsons r)
	- the 2nd highest positive correlation (high pearsons r)
	- a group with no correlation (pearsons r ~= 0)
	- the group with the lowest pearsons' correlation (lowest pearsons r)
-------------------------------   Code Section   ------------------------------
----------   Code Overview   ----------
--------   Software Versions   --------
---------------   Code   --------------
# -*- coding: utf-8 -*-
"""
Created on Thu Apr  7 07:41:45 2016

@author: e061568
"""
import pandas as pd
import scipy
import seaborn
import matplotlib.pyplot as plt
import itertools
import scipy.stats

# importing and cleaning of the database
gapminder = pd.read_csv('04_gapminder_quantitative.csv', index_col='country')
gapminder.rename(index=lambda x: str(x).lower(), inplace=True)
gapminder.dropna(inplace=True)

# doing all combinations and sorting
list_of_max_pearsons = []
for c1, c2 in itertools.combinations(gapminder.columns, 2):
	    pearson_r, p_value_two_tails = scipy.stats.pearsonr(gapminder[c1],
		                                                       gapminder[c2])
		    list_of_max_pearsons.append((pearson_r, c1, c2, p_value_two_tails))
			list_of_max_pearsons.sort(key=lambda x: x[0], reverse=True)

# taking some interesting values
pearson_r_max, c1_max, c2_max, p_max = list_of_max_pearsons[0]
pearson_r_max_2, c1_max_2, c2_max_2, p_max_2 = list_of_max_pearsons[1]
pearson_r_zero, c1_zero, c2_zero, p_zero = list_of_max_pearsons[70]
pearson_r_min, c1_min, c2_min, p_min = list_of_max_pearsons[-1]

# saving the plots into figures: 2 maximum pearsons r for GapMinder
plt.figure(1)
plt.subplot(2,1,1)
plt.xlabel(c1_max)
plt.ylabel(c2_max)
plt.text(20000, 0, (r'$r_{{pearson}}={0:0.3f}${2}'
                    '$r_{{pearson}}^2={1:0.3f}$').format(
					                    pearson_r_max, pearson_r_max ** 2,
										'\n',),
										bbox={'facecolor':'white'})
plt.title('1st Maximum Pearson\'s linear correlation for Gapminder Data Set')
plt.scatter(gapminder[c1_max], gapminder[c2_max])
plt.subplot(2,1,2)
plt.scatter(gapminder[c1_max_2], gapminder[c2_max_2])
plt.xlabel(c1_max_2)
plt.ylabel(c2_max_2)
plt.text(0, 6000, (r'$r_{{pearson}}={0:0.3f}${2}'
                   '$r_{{pearson}}^2={1:0.3f}$').format(
				                      pearson_r_max_2, pearson_r_max_2 ** 2,
									  '\n'),
									  bbox={'facecolor':'white'})
plt.title('2nd Maximum Pearson\'s linear correlation for Gapminder Data Set')
plt.tight_layout()
plt.savefig('first_and_second_highest_pearsons_correlations.png', dpi=500)
plt.close(1)
print('pearsons r: {0:0.3f}\npearsons r²: {1:0.3f}\np-value: {4:0.3f}\n'
      'for {2} and {3} data sets\n\n'.format(
	  pearson_r_max, pearson_r_max ** 2, c1_max, c2_max, p_max
	  ))
print('pearsons r: {0:0.3f}\npearsons r²: {1:0.3f}\np-value: {4:0.3f}\n'
      'for {2} and {3} data sets\n\n'.format(
	  pearson_r_max_2, pearson_r_max_2 ** 2, c1_max_2, c2_max_2, p_max_2
	  ))

# saving the plots into figures: r_pearson ~ 0
plt.figure(2)
plt.xlabel(c1_zero)
plt.ylabel(c2_zero)
plt.text(2.5e11, 30, (r'$r_{{pearson}}={0:0.3f}${2}'
                      '$r_{{pearson}}^2={1:0.3f}$').format(
					  pearson_r_zero, pearson_r_zero ** 2, '\n',
					  ),
					  bbox={'facecolor':'white'})
plt.title('Almost zero Pearson\'s $r$')
plt.scatter(gapminder[c1_zero], gapminder[c2_zero])
plt.tight_layout()
plt.savefig('pearsons_correlation_almost_zero.png', dpi=500)
plt.close(2)
print('pearsons r: {0:0.3f}\npearsons r²: {1:0.3f}\np-value: {4:0.3f}\n'
      'for {2} and {3} data sets\n\n'.format(
	  pearson_r_zero, pearson_r_zero ** 2, c1_zero, c2_zero, p_zero
	  ))

# saving the plots into figures: r_pearson_min
plt.figure(2)
plt.xlabel(c1_min)
plt.ylabel(c2_min)
plt.text(10, 80, (r'$r_{{pearson}}={0:0.3f}${2}'
                  '$r_{{pearson}}^2={1:0.3f}$').format(
				  pearson_r_min, pearson_r_min ** 2, '\n',
				  ), bbox={'facecolor':'white'})
plt.title('Pearson\'s lowest $r$ for GapMinder')
plt.scatter(gapminder[c1_min], gapminder[c2_min])
plt.tight_layout()
plt.savefig('pearsons_lowest_correlation.png', dpi=500)
plt.close(2)
print('pearsons r: {0:0.3f}\npearsons r²: {1:0.3f}\np-value: {4:0.3f}\n'
      'for {2} and {3} data sets\n\n'.format(
	  pearson_r_min, pearson_r_min ** 2, c1_min, c2_min, p_min
	  ))

-----------   Code Output   -----------

pearsons r: 0.823
pearsons r²: 0.677
p-value: 0.000
for incomeperperson and internetuserate data sets


pearsons r: 0.808
pearsons r²: 0.653
p-value: 0.000
for incomeperperson and relectricperperson data sets


pearsons r: 0.031
pearsons r²: 0.001
p-value: 0.820
for co2emissions and suicideper100th data sets


pearsons r: -0.543
pearsons r²: 0.294
p-value: 0.000
for hivrate and lifeexpectancy data sets
----------------------   Interpretation Of The Results   ----------------------
Comparison 01: incomeperperson and internetuserate
	A positive value of of 'r' indicates that the two variables have a positive correlation: the greater one of them the greater the other. The magnitude of 'r', around 80% indicates that there is a moderate to strong correlation between the variables. They do provide some information about the other when considering one of them alone.
A r² value of 0.677 indicates that the variability in the internet use rate variable can be determined 67.7% by the income per person variable. The remaining 32.3% of the variability is explained by other factors.

A significant value of p was found.

Comparison 02: income per person and relectricperperson [(2008 residential electricity consumption, per person (kWh) The amount of residential electricity consumption per person during the given year, counted in kilowatt-hours (kWh).]
	Again the reasoning is the same as the above but this time I'll be more mathmatical:
	- r > 0: positive correlation: the greater one variable, the greater the other
	- r² ~ 65%: 65% of the variability in the variable 'rate of electric energy consumption' is accounted by the variability in 'income per person'
	- | r | ~ 0.8 : there is moderate to high strenght in the correlation between the variables.

A significant value of p was found.

Comparison 03: co2emissions and suicide per 100th
	In this case both r and r² are near to zero. This indicates that the two variables considered are not linearily related. Thus we can hypothesize that either they are truly not related or that another non-linear fit can relate them.

In this case p ~ 0.8 and we cannot reject the null hypothesis. That is we can affirm that co2emissions and suicide rate per 100th are not linearily correlated.

Comparison 04: hivrate and lifeexpectancy
	In this situation the variables have a negative relationship. This means that the greater one of them the smaller the other. A 'r' absolute value of -0.54 tells that they are moderately related to one another and the r² value of ~0.3 tells that 30% of the variability in the explanatory variable can be accounted by the variability in the response variable (and vice versa).

A significant value of p was found.
